{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DH Downunder: Distant Reading\n",
    "\n",
    "## Notebook 3: Stylistics, or Computer Magic\n",
    "\n",
    "One of the most interesting applications of distant reading techniques is authorship attribution. In this third and final notebook for our Distant Reading course, we will consider how text analysis can be used to work out the true identity of a text's author.\n",
    "\n",
    "Authorship attribution will also give us a chance to introduce several concepts from data science, such as vectors, distance metrics and clustering. These concepts are useful in a number of areas beyond authorship attribution.\n",
    "\n",
    "Execute the cell below to load all the packages required for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random # some useful randomisation functions\n",
    "\n",
    "import numpy as np # for doing maths\n",
    "import nltk # for natural language processing functions\n",
    "import matplotlib # for displaying graphs\n",
    "import scipy # contains useful scientific formulas and convenience functions\n",
    "import pandas as pd # for working with tabular data (like Excel, but a trillion times better)\n",
    "\n",
    "from import_corpus import import_corpus\n",
    "\n",
    "# So graphs render properly\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Load and Inspect our Corpus\n",
    "\n",
    "For this notebook, I have pre-prepared a corpus for you. Execute the cell below to import all the novels into your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "novels = import_corpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one mystery novel in our corpus! In 1814, the novel *Waverley, or 'tis Sixty Years Since* was published anonymously. In this session we are going to use statistical analysis to find out who ??? really was.\n",
    "\n",
    "The whole corpus, `novels` is a single `dict`. Execute the cell below to see how to access information from the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_keys = '\\n  -  '.join(list(novels))\n",
    "novel_keys = '\\n  -  '.join(list(novels['waverley']))\n",
    "print(f'The keys for the novels dict are:\\n  -  {corpus_keys}\\n\\n')\n",
    "\n",
    "print(f'Some examples of how to find information:\\n')\n",
    "print(f'The title and author of Castle Rackrent:')\n",
    "print(f'novels[\"rackrent\"][\"title\"] = {novels[\"rackrent\"][\"title\"]}')\n",
    "print(f'novels[\"rackrent\"][\"author\"] = {novels[\"rackrent\"][\"author\"]}')\n",
    "print('\\nTokens 1000-1009 of The Hungry Stones:')\n",
    "print(f'novels[\"hungry_stones\"][\"tokens\"][1000:1010] = {novels[\"hungry_stones\"][\"tokens\"][1000:1010]}')\n",
    "print(f'\\nFor each novel, the following information is available:\\n  -  {novel_keys}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will notice that the body text and tokens have been put in lowercase, as discussed in our previous sessions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Calculate word frequencies for each novel\n",
    "\n",
    "One of the most interesting findings of distant reading over the last few decades has been that each person has a detectable stylistic 'signature'. It seems that when we write, each of us uses the extremely common words, 'the', 'a', 'and', 'for', 'but' in a particular ratio which is more-or-less unique. In the most famous paper in the field of stylometry, John Burrows showed that we can use this fact to help identify the authors of disputed texts:\n",
    "\n",
    "* John Burrows, [‘Delta’: a Measure of Stylistic Difference and a Guide to Likely Authorship](https://doi.org/10.1093/llc/17.3.267), *Literary and Linguistic Computing* (2002) 17: 267-87.\n",
    "\n",
    "There are many other measurements of stylistic similarity, and Burrows himself argues that 'Delta' is not sufficient on its own, but it is a good place to start the study of authorship attribution. In this notebook, you will learn exactly how to calculate Burrows' Delta and use it to find out who wrote *Waverley*.\n",
    "\n",
    "The first step is to calculate the word frequencies for all the words in each novel. How often does each writer use each of the words in their vocabulary?\n",
    "\n",
    "We can use Python's `set()` object and the `.count()` list method to do this. A `set()` object is a collection of *unique* values. Execute the cell below to see how a `set()` can be used to get the vocabulary of a text from a list of tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_list_of_tokens = ['hello','my','name','is','Han','Solo','hello','my','name','is','Luke','Skywalker']\n",
    "\n",
    "a_set_of_tokens = set(a_list_of_tokens) # Conver the list into a set.\n",
    "\n",
    "print(f'Here is the Set you just created from a_list_of_tokens: {a_set_of_tokens}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new `set` now contains exactly one copy of each word that appears in the original `list`. Notice for example that there is only one 'hello', one 'name' and one 'is'. Sets do not preserve the order of the original values, but this is immaterial for our purposes.\n",
    "\n",
    "Notice that the new `set` is enclosed in curly braces `{}`. This is an unfortunate abiguity in Python, since `dicts` are also enclosed in curly braces `{}`. If you are ever unsure whether you are dealing with a `set` or a `dict`, you can use the `type` function to check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(a_set_of_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now we can quickly and easily find out what all the words are in a given novel. Now we need to work out how to count all those words. The `.count()` method gives us a simple way to do this. Execute the cell below to see how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "another_list_of_tokens = ['James','James','Morrison','Morrison','Wetherby','George','Dupree',\n",
    "                         'took','great','care','of','his','mother',\n",
    "                         'though','he','was','only','three',\n",
    "                         'James','James','Morrison','Morrison','said','to','his','mother','said','he',\n",
    "                         'you','must','never','go','down','to','the','end','of','the','town',\n",
    "                         'if','you','don\\'t','go','down','with','me']\n",
    "\n",
    "n_james = another_list_of_tokens.count('James')\n",
    "n_of = another_list_of_tokens.count('of')\n",
    "\n",
    "print(f'In the opening stanza of this lovely children\\'s poem, \"James\" appears {n_james} times, and \"of\" appears {n_of} times.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The one other important point is that texts can vary considerably in length. Two books may both use the word 'the' 2000 times, but if book A is 10,000 words long, and book B is 10,000,000 words long, then obviously these 2000 'the's would mean very different things!\n",
    "\n",
    "The usual way to express word frequencies is therefore 'occurrences per 1000 words'. The formula for calcuating this is $$\\frac{n_x}{t} \\times 1000 $$ where $n_x$ is the number of times word $x$ appears in the text and $t$ is the total words in the text.\n",
    "\n",
    "### Exercise 2.1: Compute the relative word frequency of 'scotland'\n",
    "\n",
    "Complete the code in the cell below to calculate the frequency per 1000 words of the word 'scotland' in *Waverley* (remember all words have been put into lower case). You can use `len()`, `.count('word')`, `*` and `/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "waverley_tokens =          # Get Waverley's tokens from the 'novels' dict\n",
    "scotland_count =           # Count the number of times 'scotland' is used\n",
    "total_words =              # Count the total number of words in the novel\n",
    "rel_freq_scotland =        # Apply the formula above\n",
    "\n",
    "# END OF YOUR CODE\n",
    "\n",
    "print(f'The word \"scotland\" appears {rel_freq_scotland:.3f} times per 1000 words in Waverley.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output:** The word \"scotland\" appears 0.509 times per 1000 words in Waverley."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2: Find the 20 most common words in *Waverley*\n",
    "\n",
    "Now let's find the 20 most common words in the novel. You can use `set()` on the list of *Waverley*'s tokens to get the vocabulary of the novel, then you can use a `for` loop to look for each word in the novel and apply the formula above. I have provided the code that will fetch the top twenty words when you are done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "waverley_tokens =             # Get Waverlery's tokens from the 'novels' dict\n",
    "total_words =                 # Use len() to get the total words\n",
    "waverley_vocab =              # Use set() to get all the unique words\n",
    "\n",
    "results = {}                  # Initialise results dict (done for you)\n",
    "\n",
    "for word in waverley_vocab:   # Loop over the set of unique words (done for you)\n",
    "    \n",
    "    word_count =              # How many times does this word appear in the novel?\n",
    "    \n",
    "    per_1000 =                # How many times does this word appear per 1000 words in the novel (use formula)\n",
    "    \n",
    "    results[word] = per_1000  # Add the result to the results dict (done for you)\n",
    "\n",
    "# END OF YOUR CODE\n",
    "\n",
    "# Sort the results:\n",
    "top_20 = [(k, results[k]) for k in sorted(results, key=results.get, reverse=True)][0:20]\n",
    "\n",
    "# Display them:\n",
    "top_20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we make any guess about the author based on this information?\n",
    "\n",
    "Obviously this method works, but it is very slow. Many programming libraries contain fast, useful functions for just this sort of task. One useful one is `FreqDist()`, which is provided by the Natural Langauge Tooklit. It has a very useful feature, the `.most_common()` method, which you can use to extract the most common words in a text. Execute the cell below to see how this function works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "\n",
    "waverley_freqs = FreqDist(novels['waverley']['tokens'])\n",
    "\n",
    "waverley_freqs.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately `FreqDist()` gives you raw frequencies. A more advanced function is the `CountVectorizer()` function from the scikit-learn package. This function takes a `list` of strings, and outputs a \"Document-Term Matrix\" (DTM). A DTM is a giant table, where each row represents a novel from your corpus, and each column is a particular word:\n",
    "\n",
    "|Novel|aaron|ab|aback|abacus|abaddon|abana|abandon|...|\n",
    "|---|---|---|---|---|---|---|---|---|\n",
    "|wyllards_weird|0|0|0|0|0|0|5|...|\n",
    "|rackrent|0|0|0|0|0|0|0|...|\n",
    "|hungry_stones|0|0|1|0|0|0|1|...|\n",
    "|talisman|0|0|0|1|0|0|3|...|\n",
    "|ivanhoe|2|0|0|2|0|1|6|...|\n",
    "|...|...|...|...|...|...|...|...|...|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# CountVectorizer requires us to provide a list, where each item of the list is a text represented\n",
    "# by a single string. So first we will extract all the texts into text_list, and keep track of which\n",
    "# text is which using novel_list.\n",
    "novel_list = []\n",
    "text_list = []\n",
    "for name, novel in novels.items():\n",
    "    novel_list.append(name)\n",
    "    text_list.append(novel['body'])\n",
    "    \n",
    "# Initialise the 'vectorizer', the object we can use to convert the novels into a DTM:\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Apply the vectorizer to the novels. The .fit_transform method performs the two steps we performed above:\n",
    "# fit = find the vocabulary of all the texts\n",
    "# transform = count the instances of each vocabulary word in each text\n",
    "DTM = vectorizer.fit_transform(text_list)\n",
    "\n",
    "print(f'The columns of the below matrix represent: {\", \".join(vectorizer.get_feature_names()[63:70])}\\n')\n",
    "row_names = \"\\n\".join(novel_list[0:5])\n",
    "print(f'The rows represent:\\n{row_names}\\n')\n",
    "print(DTM.toarray()[0:5,63:70])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, these are also raw frequencies, and we would like to have the relative frequencies. This is very easy to compute using the built-in `.sum()` method. The object we have just created, `DTM`, is a [csr sparse matrix](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html), which has many useful built-in methods besides `.sum()`. \n",
    "\n",
    "Since each *row* represents one novel, if we add up all the numbers in a single *row*, we will get the total words for that novel.\n",
    "\n",
    "### Exercise 2.3: Calculate the relative frequencies.\n",
    "\n",
    "In the cell below, use the `.sum()` method to calculate the sum for each row. Then \n",
    "\n",
    "**NB:** You have to use the extra parameter `axis = 1` when you call the `.sum()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "row_sums =        # Use .sum(axis = 1) to find the row sums of the DTM\n",
    "DTM =             # Divide the whole DTM by row_sums and then multiply the DTM by 1000 to get the relative frequencies\n",
    "\n",
    "# END OF YOUR CODE\n",
    "\n",
    "DTM = np.array(DTM) # Type conversion to deal with quirk in software\n",
    "\n",
    "print(f'The columns of the below matrix represent: {\", \".join(vectorizer.get_feature_names()[63:69])}\\n')\n",
    "row_names = \"\\n\".join(novel_list[0:5])\n",
    "print(f'The rows represent:\\n{row_names}\\n')\n",
    "print(DTM[0:5,63:69])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NB:** Due to a quirk in the software, `DTM` has now been converted into a [numpy array](https://docs.scipy.org/doc/numpy/reference/generated/numpy.array.html). This is a very similar data type to a csr sparse matrix, so don't worry too much about it. The only difference is that you will see the `keepdims = True` argument popping up now and again, which is not necessary when you are working with a sparse matrix.\n",
    "\n",
    "Since we have now divided each row by the total words, and multiplied by 1000, each row now adds up to 1000:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTM.sum(axis = 1, keepdims = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Calculate \"Burrows' Delta\" between Waverley and the other novels\n",
    "\n",
    "Now we have all the tools we need to calculate Burrows' Delta. There are two main steps:\n",
    "\n",
    "* Perform 'mean normalisation'\n",
    "* Compute the 'distance' between the different novels.\n",
    "\n",
    "'Mean normalisation' is necessary so that the highest-frequency words do not have any excessive effect on the calculation. The formula is:\n",
    "\n",
    "$$ \\frac{X - \\mu}{\\sigma} $$\n",
    "\n",
    "Where $X$ is the word frequencies, $\\mu$ is the mean score for each word, and $\\sigma$ is the standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.1: Choose which words to include\n",
    "\n",
    "Burrows' Delta only gives reliable results if you focus on the most common words in the corpus. Usually the 100-200 most common words are the most useful. In the next cell, you will create a 'mask' that you can use to just keep the columns of the `DTM` that you want.\n",
    "\n",
    "To do this, you first need to calculate a threshold: the frequency of the nth-most frequent word in the corpus. Then you can use a the logical function `>=` (greater than or equal to) to find the columns whose total frequency is highest. You can then use this mask to keep just the columns you want with the command `DTM[:,mask]`.\n",
    "\n",
    "* Compute the sum of frequencies for each word using `.sum()` with the additional argument `axis = 0`.\n",
    "* Sort the sums with the function `np.argsort()`.\n",
    "* This sorts the numbers in ascending order. To put the high-frequency numbers first, use `np.flip()`.\n",
    "* Find the columns index of the 200th word. **NB:** Remember that Python counts from zero!\n",
    "* Use the index to calculate the threshold\n",
    "* Use the `>=` operation to create the mask you can use to just keep the columns of the most frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "col_sums =           # Use np.sum() on 'DTM' with additional argument 'axis = 0'\n",
    "cols_asc =           # Use np.argsort() on col_sums to sort them into ascending order\n",
    "cols_desc =          # Flip them using np.flip(), so they are now in descending order\n",
    "idx_200 =            # Find the index of the 200th-highest column, usinc cols_desc[]\n",
    "threshold =          # Use idx_200 to find the threshold for the top 200 words\n",
    "mask =               # Use '>=' with the threshold to create the mask\n",
    "\n",
    "# END OF YOUR CODE\n",
    "\n",
    "print(f'If the mask worked, then DTM_masked should have shape (22,200):\\n')\n",
    "print(f'Shape of DTM_masked = DTM[,mask].shape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "col_sums =           # Use np.sum() on 'DTM' with additional argument 'axis = 0'\n",
    "cols_asc =           # Use np.argsort() on col_sums to sort them into ascending order\n",
    "cols_desc =          # Flip them using np.flip(), so they are now in descending order\n",
    "idx_200 =            # Find the index of the 200th-highest column, usinc cols_desc[]\n",
    "threshold =          # Use idx_200 on col_sums find the threshold for the top 200 words\n",
    "mask =               # Use '>=' with the threshold and col_sums to create the mask\n",
    "\n",
    "# END OF YOUR CODE\n",
    "\n",
    "print(f'If the mask worked, then DTM[:,mask] should have shape (22, 200):\\n')\n",
    "print(f'Shape of DTM_masked = {DTM[:,mask].shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.2: Mean normalise the word frequencies.\n",
    "\n",
    "To do this, you need to calculate the mean for each column (i.e. for each word) of `DTM`, calculate the standard deviation for each column of `DTM`, and then apply the formula. Some functions that you will need:\n",
    "\n",
    "* `np.mean()`: [This function](https://docs.scipy.org/doc/numpy/reference/generated/numpy.mean.html) will calculate the mean for each column. Remember to use `axis = 0` and `keepdims = True`!\n",
    "* `np.std()`: [This function](https://docs.scipy.org/doc/numpy/reference/generated/numpy.std.html) will calculate the standard deviation for each column. Remember to use `axis = 0` and `keepdims = True`.\n",
    "* If you have used this functions correctly, then you should be able to mean normalise the entire `DTM` in a single go using the normal matematical operators, `-` and `/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "col_means =         # Use np.mean() on your masked DTM, with the additional arguments axis = 0 and keepdims = True\n",
    "col_sd =            # Use np.std() on your masked DTM, with the additional arguments axis = 0 and keepdims = True\n",
    "DTM_norm =          # Apply the formula: (X - mean) / standard_deviation. X = your masked DTM\n",
    "\n",
    "# END OF YOUR CODE\n",
    "\n",
    "print(f'If the formula worked, then the mean of each column should now be zero...')\n",
    "i = random.randint(0,DTM_norm.shape[1]-1) # Choose a random column\n",
    "print(f'The mean of column {i} ({vectorizer.get_feature_names()[i]}) is {np.mean(DTM_norm[:,i]):.4f}.')\n",
    "print(\"There may be a minus sign due to numerical roundoff.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.3: Calculate the euclidean distance between the different books\n",
    "\n",
    "The 'euclidean distance' is a generalisation of Pythagoras' theorem. It is calculated using the following formula:\n",
    "\n",
    "$$\\delta = \\sqrt{x_1^2 + x_2^2 + x_3^2 + \\ldots + x_n^2}$$\n",
    "\n",
    "where $x_n$ is the frequency of word $n$ in the given novel. In this case, the symbol $\\delta$ ('delta') is used for the measurement, because that is the name given to it by John Burrows (actually, he used a slightly different measurement of distance, the so call 'cityblock' or 'manhattan' distance, in the original paper).\n",
    "\n",
    "We could implement this function ourselves, but there is an extremely fast and convenient function already available in the `scipy` package, the `pdist` function. It will quickly calculate the euclidean distance between every novel and every other novel in the matrix. It returns its output in a difficult to use form, but the `squareform` function, also form `scipy`, can be used to reshape the data into a matrix like so:\n",
    "\n",
    "$$\\matrix{\\text{}&novel_0&novel_1&\\ldots &novel_n\\cr\n",
    "                novel_0& 0 & \\delta_{01} & \\ldots & \\delta_{0n}\\cr\n",
    "                novel_1& \\delta_{10}  &  0 & \\ldots & \\delta_{1n}\\cr\n",
    "                \\ldots& \\vdots & \\vdots & \\ddots & \\vdots\\cr\n",
    "                novel_n& \\delta_{n0}  & \\delta_{n1} &\\ldots & 0}$$\n",
    "\n",
    "Now $\\delta_{12}$, the distance between novel 1 and novel 2, will obviously be exactly equal to $\\delta_{21}$, the distance between novel 2 and novel 1. So the upper and lower triangles of the matrix are mirror-images of one another. Also, the distance between any novel and itself will be 0, because it is identical to itself. So the diagonal of the matrix is a row of zeros. For these reasons, it may seem strange to use a matrix to store the data—there is such wasted space! But it does make the data easy to access. All you need to do to find the distance between novels 7 and 21 is to type `distance_matrix[7,21]` (or `distance_matrix[21,7]`, it doesn't matter.) Likewise, to find the distances between novel 0 and all the other novels in the corpus, you would just type `distance_matrix[0,:]` or `distance_matrix[:,0]`.\n",
    "\n",
    "In the cell below:\n",
    "\n",
    "1. Use the `pdist()` function on `DTM_norm` with your `mask` to calculate the euclidean distances between all the novels on the most common words. (Hint: You can apply a *column* mask to a matrix like so: `matrix[:,mask]`.)\n",
    "2. Reshape the result using `squareform()`.\n",
    "3. Find the distance between novel 2 and novel 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "distance_vector = pdist(DTM_norm[:,mask])         # Use pdist() on DTM_norm with 'metric = euclidean'. Remember your mask!\n",
    "distance_matrix = squareform(distance_vector)         # Use squareform() to reshape the result\n",
    "two_and_twenty = distance_matrix[2,20]          # What is the distance between novels two and twenty?\n",
    "\n",
    "# END OF YOUR CODE\n",
    "\n",
    "n_2 = novel_list[2]\n",
    "n_20 = novel_list[20]\n",
    "print(f'Novel 2 is {novels[n_2][\"title\"]}, by {novels[n_2][\"author\"]}.')\n",
    "print(f'Novel 20 is {novels[n_20][\"title\"]}, by {novels[n_20][\"author\"]}.')\n",
    "print(f'The distance between them is {two_and_twenty:3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Who wrote *Waverley*?\n",
    "\n",
    "So who wrote *Waverley*? Execute the cell below to find out...\n",
    "\n",
    "(**NB:** We can use the 'pandas' library to put the results in a useful table.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get index for Waverley:\n",
    "idx = novel_list.index('waverley')\n",
    "\n",
    "# Get distance measurements\n",
    "waverley_deltas = distance_matrix[idx,:]\n",
    "\n",
    "# Create table to display results\n",
    "titles = [novels[x]['title'] for x in novel_list]\n",
    "authors = [novels[x]['author'] for x in novel_list]\n",
    "\n",
    "waverley_data_frame = pd.DataFrame({\n",
    "    'Novel':titles,\n",
    "    'Author':authors,\n",
    "    'Delta':waverley_deltas\n",
    "})\n",
    "\n",
    "\n",
    "# The moment of truth... Display the table:\n",
    "waverley_data_frame.sort_values('Delta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should go back up and try changing how many words you consider. Are the results as accurate when you only consider the top 50 or top 30 words? What about if you conside the top 1000? Or all the words?\n",
    "\n",
    "\n",
    "### A final bit of fun:\n",
    "\n",
    "The `scipy` package has a fun and useful function, `linkage`, which you can use to create a 'dendrogram' visualising the relationships between all the texts in your corpus. Execute the cell below to check it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "linkage_matrix = linkage(distance_vector)\n",
    "plt.figure()\n",
    "dendro = dendrogram(linkage_matrix)\n",
    "\n",
    "print(\"Walter Scott's novels are:\")\n",
    "scott_novels = [(novel_list.index(key), value[\"title\"]) for key,value in novels.items() if value[\"author\"] == \"Sir Walter Scott\"]\n",
    "\n",
    "for x in scott_novels:\n",
    "    print(f'Novel {x[0]}: {x[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Thank you for attending this course. This repository of code and data is yours to keep. May it help you in your own reserach, and may you outgrow it in all rapidity!\n",
    "\n",
    "Michael Falk, Western Sydney University"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
