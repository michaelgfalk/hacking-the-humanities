{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# DH Downunder: Distant Reading\n",
    "\n",
    "## Notebook 2: Large Corpora, Regular Expressions, and the NLTK\n",
    "\n",
    "**Session 2: Tuesday 4th December, 11:00-12:30**\n",
    "\n",
    "Welcome back to *Distant Reading*. In this notebook, we will build on your skills in Python and introduce the Natural Language Toolkit, a powerful set of tools for working with textual data.\n",
    "\n",
    "Hopefully you have already installed the Natural Language ToolKit, as instructed in the 'Getting Started' section of this repository. If you installed Python using Anaconda, then you already have the NLTK. If not, then you need to open a new Terminal (Mac) or Command Prompt window (Windows), and type the following:\n",
    "```\n",
    "python -m pip install --upgrade pip\n",
    "pip install nltk\n",
    "```\n",
    "If you have installed the NLTK, you then need to install some of the data that it uses for its more advanced functions. To do this, run the cell below. It may take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /Users/michaelgfalk/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /Users/michaelgfalk/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /Users/michaelgfalk/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /Users/michaelgfalk/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /Users/michaelgfalk/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /Users/michaelgfalk/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /Users/michaelgfalk/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/names.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /Users/michaelgfalk/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /Users/michaelgfalk/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /Users/michaelgfalk/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /Users/michaelgfalk/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     /Users/michaelgfalk/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /Users/michaelgfalk/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /Users/michaelgfalk/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /Users/michaelgfalk/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/words.zip.\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /Users/michaelgfalk/nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /Users/michaelgfalk/nltk_data...\n",
      "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /Users/michaelgfalk/nltk_data...\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /Users/michaelgfalk/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('popular')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Section 1: Using lists and dicts to store data\n",
    "\n",
    "So far you have been implementing Python commands one at a time. This will obviously become very tedious if you have to deal with more than two or three texts. What if you want to run a stylometric analysis on all 6 of Jane Austen's novels, or all 32 of Shakespeare's plays, or on a historical corpus of thousands of letters or millions of newspaper articles?\n",
    "\n",
    "To work with a corpus, we are going to learn to use two new features of Python: `for` loops, which will allow us to apply code to multiple books at once, and `dicts`, which will allow us to store information about our books in a convenient format.\n",
    "\n",
    "Let's start with the `dict`. So far you are familiar with one main data type, the `list`. A `list` simply stores a bunch of things in an order, and you can fetch them using the numerical index, as in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "my_list = [7, 8, 9]\n",
    "print(my_list[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "A `dict` is different. It stores information in `key:value` pairs. You get information out of a dict by referring to a `key`. You can get a list of all the keys in a `dict` by using the `.keys()` method. Execute the cell below to see this in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$0.30\n",
      "dict_keys(['colour', 'type', 'price'])\n"
     ]
    }
   ],
   "source": [
    "orange = {'colour':'orange','type':'citrus','price':'$0.30'}\n",
    "print(orange['price'])\n",
    "print(orange.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Exercise 1.1: Extract information from a `dict`\n",
    "\n",
    "In the cell below, I have provided you a `dict` containing information about Arjuna, the hero of the great epic poem, *Mahabharata*. Use the `dict` to find out what family Arjuna is from, and use the `.keys()` method to find out what else we know about him."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arjuna belongs to the Pandavas.\n",
      "We know the following things about Arjuna: ['gender', 'species', 'family', 'profession']\n"
     ]
    }
   ],
   "source": [
    "arjuna = {'gender':'M', 'species':'demigod', 'family':'Pandavas', 'profession':'archer'}\n",
    "\n",
    "# YOUR CODE HERE\n",
    "arjunas_family = arjuna['family']\n",
    "\n",
    "arjuna_keys = arjuna.keys()\n",
    "# END OF YOUR CODE\n",
    "\n",
    "print(f'Arjuna belongs to the {arjunas_family}.')\n",
    "print(f'We know the following things about Arjuna: {list(arjuna_keys)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "When we ingest our corpus, we will store information about each text in a `dict`. But how will we store information about multiple novels? Well, it is possible to store multiple `dicts` in a single `list`. Execute the cell below to see how this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One apple costs $0.15.\n",
      "A pear is an example of a pome fruit.\n"
     ]
    }
   ],
   "source": [
    "fruits = [\n",
    "    {'name':'orange','colour':'orange','type':'citrus','price':'$0.30'},\n",
    "    {'name':'apple','colour':'red','type':'pome','price':'$0.15'},\n",
    "    {'name':'pear','colour':'green','type':'pome','price':'$0.10'},\n",
    "    {'name':'bluberry','colour':'dark blue','type':'berry','price':'$0.05'}\n",
    "]\n",
    "\n",
    "apple_price = fruits[1]['price'] # the apple is the second fruit in the list\n",
    "pear_type = fruits[2]['type'] # the pear is the third fruit in the list\n",
    "\n",
    "print(f'One apple costs {apple_price}.')\n",
    "print(f'A pear is an example of a {pear_type} fruit.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Exercise 1.2: Extract information from a list of dicts\n",
    "\n",
    "In the cell below, I have provided you a `list` of all the members of the Pandava family. Execute the cell to import the `list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from pandavas import pandavas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Use this list to find out the `name` and `spouse` of the second Pandava in the list and the fifth Pandava.\n",
    "\n",
    "*Remember:* Python starts counting from 0, not from 1.\n",
    "\n",
    "*Hint:* If you're stuck, and want to have a look at the structure of the list, try creating a new cell, and then executing one of these commands:\n",
    "\n",
    "```python\n",
    "pandavas\n",
    "```\n",
    "or\n",
    "```python\n",
    "pandavas[0].keys()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Bhima',\n",
       " 'gender': 'M',\n",
       " 'species': 'demigod',\n",
       " 'family': 'Pandavas',\n",
       " 'profession': 'mace wielder',\n",
       " 'spouse': 'Draupadi'}"
      ]
     },
     "execution_count": 13,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandavas[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The second-eldest Pandava was Bhima. He was married to Draupadi.\n",
      "The fifth Pandava was Sahadeva. He was married to Draupadi.\n",
      "There were 6 Pandava brothers.\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "second_name = pandavas[1]['name']\n",
    "second_spouse = pandavas[1]['spouse']\n",
    "\n",
    "fifth_name = pandavas[4]['name']\n",
    "fifth_spouse = pandavas[4]['spouse']\n",
    "\n",
    "num_pandavas = len(pandavas)\n",
    "# END OF YOUR CODE\n",
    "\n",
    "print(f'The second-eldest Pandava was {second_name}. He was married to {second_spouse}.')\n",
    "print(f'The fifth Pandava was {fifth_name}. He was married to {fifth_spouse}.')\n",
    "print(f'There were {num_pandavas} Pandava brothers.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "That's right. The Pandavas had [interesting marriage practices](https://en.wikipedia.org/wiki/Pandava). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Exercise 1.3: Create and update your own `dict`.\n",
    "\n",
    "You know now how to get information out of a dict, but how do you create a `dict`, or add new information to a `dict` that already exists? Both of these tasks are easy. To create a `dict`, you use curly braces `{}`, and simply enter the keys and values like so: `{key:value, key:value, key:value}`. You use a colon `:` to join a `key` to a `value`, and commas to seperate different `key:value` pairs from one another.\n",
    "\n",
    "**NB:** If your `keys` or `values` are words, they must be in inverted commas or quotation marks, e.g. `{'name':'Jane' ... }`. If they are numbers, you should leave the quotation marks off, e.g. `{'age':30}`.\n",
    "\n",
    "In the cell below, create a `dict` describing you favourite book or movie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "my_favourite = {\n",
    "    'title': ,\n",
    "    'writer': ,\n",
    "    # ... add whatever extra key:value pairs you like\n",
    "}\n",
    "# END OF YOUR CODE\n",
    "my_favourite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "To update a `dict`, simply select which piece of information you would like to change, and then use the 'assignment operator', `=`, to set the new value, e.g.\n",
    "\n",
    "```\n",
    "orange['price'] = new_price\n",
    "```\n",
    "\n",
    "In the next cell, change something about your favourite book or movie, e.g. its title:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "my_favourite[ ] = \n",
    "# END OF YOUR CODE\n",
    "my_favourite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Section 2: Using a `for`-loop to import your corpus.\n",
    "\n",
    "Now you are a master of `lists` and `dicts`, it is time to learn how to use a `for`-loop to import your entire corpus.\n",
    "\n",
    "There are different ways to use `for`-loops in Python. In this notebook, we will learn the simplest way, which is to apply a `for`-loop to a list. What a `for`-loop lets you do is apply a piece of code to each item in the list. Execute the cell below to see how this works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new number is 9.\n",
      "The new number is 14.\n",
      "The new number is 86.\n",
      "The new number is 8.\n",
      "The new number is 2.\n"
     ]
    }
   ],
   "source": [
    "my_list = [7, 12, 84, 6, 0]\n",
    "\n",
    "for number in my_list:\n",
    "    new_number = number + 2\n",
    "    print(f\"The new number is {new_number}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Let's break down the syntax. The first line tells Python to loop over `my_list`. Since this is a list of numbers, we're going to use the word `number` to refer to each item in the list:\n",
    "```\n",
    "for number in my_list:\n",
    "```\n",
    "You could use a different word, e.g. `x` or `num` or `supercalafragilisticexpealadocious`. It doesn't matter what you pick, so long as it makes sense to you and you are consistent.\n",
    "\n",
    "In order for the `for`-loop to work, the next lines must be indented. Since we have used the word `number` to refer to each item in `my_list`, we must use that word in our code that we are applying to the list. This line of code tells Python to look at the `number` we are up to in the list and add two to it:\n",
    "```\n",
    "    new_number = number + 2\n",
    "```\n",
    "\n",
    "\n",
    "### Exercise 2.1: Use a for-loop to turn a list of strings to lower case\n",
    "\n",
    "In the cell below, use a `for`-loop to turn all the upper case words to lower case using the `.lower()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new word is the.\n",
      "The new word is cat.\n",
      "The new word is sat.\n",
      "The new word is on.\n",
      "The new word is the.\n",
      "The new word is mat.\n"
     ]
    }
   ],
   "source": [
    "upper_case_list = ['THE','CAT','SAT','ON','THE','MAT']\n",
    "\n",
    "# YOUR CODE HERE:\n",
    "for arbitrary_name in upper_case_list :\n",
    "    new_word = arbitrary_name.lower()\n",
    "    \n",
    "# END OF YOUR CODE\n",
    "    print(f'The new word is {new_word}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Exercise 2.2: Use a for-loop to add items to an empty list\n",
    "\n",
    "Now we are going to learn a common technique for importing and storing data in Python. So far in our for-loops we have simply printed the results so we can see what we have done. But we would ideally like to store the results of our code. To do this, we create an empty list, like so:\n",
    "```\n",
    "empty_list = []\n",
    "```\n",
    "and then we can add to it by using the `.append()` method:\n",
    "```\n",
    "empty_list.append(new_thing)\n",
    "```\n",
    "This `.append()` method lets us add a new item to our empty list in each iteration of the for-loop. **NB:** When you use `.append()`, you do not need to use the assignment operator, `=`.\n",
    "\n",
    "In the cell below you have two tasks.\n",
    "1. Create an empty list.\n",
    "2. Loop over `number_list`. Create a new number using a mathematical operation (`+`,`-`,`*`,`/`), and store the new number in the empty list using `.append()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New number apppended! The empty_list is now: [162].\n",
      "New number apppended! The empty_list is now: [162, 231].\n",
      "New number apppended! The empty_list is now: [162, 231, 69].\n",
      "New number apppended! The empty_list is now: [162, 231, 69, 12].\n",
      "New number apppended! The empty_list is now: [162, 231, 69, 12, 270].\n",
      "New number apppended! The empty_list is now: [162, 231, 69, 12, 270, 243].\n"
     ]
    }
   ],
   "source": [
    "number_list = [54,77,23,4,90,81]\n",
    "\n",
    "# YOUR CODE HERE:\n",
    "empty_list = []\n",
    "\n",
    "# Loop over the list of numbers:\n",
    "for number in number_list :\n",
    "    # Calculate the new number:\n",
    "    new_number = number * 3\n",
    "    # Append it to the empty list:\n",
    "    empty_list.append(new_number)\n",
    "\n",
    "# END OF YOUR CODE\n",
    "    print(f'New number apppended! The empty_list is now: {empty_list}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Section 3: Bringing it all together\n",
    "\n",
    "Now you know how to create a dict, how to do a for loop and how to add items to an empty list. From the previous session, you know how to import a text file using `open('path/to/file.txt', 'r')` and `.read()`.\n",
    "\n",
    "Armed with this knowledge, you are going to get a list of file names, loop over them and create a `dict` for each text in the corpus.\n",
    "\n",
    "The cell below uses the `listdir()` function from the `os` module to get a list of all the files in the `corpus` folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['erewhon.txt',\n",
       " 'moby_dick.txt',\n",
       " 'gitanjali.txt',\n",
       " 'corinne.txt',\n",
       " 'pilgrims_progress.txt',\n",
       " 'pride_and_prejudice.txt',\n",
       " 'father_goriot.txt',\n",
       " 'north_and_south.txt',\n",
       " 'such_is_life.txt',\n",
       " 'jane_eyre.txt']"
      ]
     },
     "execution_count": 20,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "file_list = os.listdir('corpus') # get list of filenames in 'corpus' folder\n",
    "file_list # print the list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This is a fairly small corpus of 10 texts. But once you know how to import a corpus of 10 texts, exactly the same code will easily import a corpus of 10,000 or 10,000,000.\n",
    "\n",
    "### Exercise 3.1: Import the corpus\n",
    "\n",
    "Complete the code in the for-loop below, so that Python will loop over all the files in the `file_list`, create a dict for each novel, and append it to `novel-list`. Each novel will be a `dict` with two `key:value` pairs, like so:\n",
    "```\n",
    "{'title':'moby_dick.txt','text':'Chapter 1: Call me Ishmael ...'}\n",
    "```\n",
    "Remember, to read from a file in Python, you must do the following:\n",
    "```\n",
    "path = 'path/to/your/file.txt'\n",
    "with open(path, 'r', encoding = 'utf-8') as file:\n",
    "    my_var = file.read()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus imported. There are 10 novels in the corpus.\n",
      "\n",
      "The third novel is gitanjali.txt. The first 200 characters are:\n",
      "\n",
      " ***﻿The Project Gutenberg EBook of Gitanjali, by Rabindranath Tagore\n",
      "\n",
      "This eBook is for the use of anyone anywhere at no cost and with\n",
      "almost no restrictions whatsoever.  You may copy it, give it away...\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE:\n",
    "\n",
    "# First create the empty list to store your corpus:\n",
    "novel_list = []\n",
    "\n",
    "# Now loop over 'file_list':\n",
    "for file_name in file_list :\n",
    "    # Create a new dict for the current novel in the list (done for you)\n",
    "    current_novel = {}\n",
    "    \n",
    "    # Set the title to be the file name \n",
    "    current_novel['title'] = file_name\n",
    "    \n",
    "    # Now open the file of the novel.\n",
    "    # First add 'corpus/' to the file name, so that Python knows where to find the file:\n",
    "    file_path = 'corpus/' + file_name\n",
    "    \n",
    "    # Then open the file and read it:\n",
    "    with open(file_path, encoding='utf-8', mode='r', errors='ignore') as novel_file:\n",
    "        # Now the file is open, use the '.read()' method to get the text\n",
    "        current_novel['text'] = novel_file.read()\n",
    "    # Finally, append the current novel to the novel_list:\n",
    "    novel_list.append(current_novel)\n",
    "    \n",
    "# END OF YOUR CODE\n",
    "\n",
    "print(f'Corpus imported. There are {len(novel_list)} novels in the corpus.\\n')\n",
    "print(f'The third novel is {novel_list[2][\"title\"]}. The first 200 characters are:\\n\\n {novel_list[2][\"text\"][0:200]}...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Section 4: Cleaning the data using regular expressions\n",
    "\n",
    "You might have noticed that the texts in your corpus begin with very similar words:\n",
    "```\n",
    "print(novel_list[5][\"text\"][0:200])\n",
    "\n",
    "The Project Gutenberg EBook of Moby Dick; or The Whale, by Herman\n",
    "Melville\n",
    "\n",
    "This eBook is for the use of anyone anywhere at no cost and with almost\n",
    "no restrictions whatsoever.  You may copy it, give\n",
    "```\n",
    "They also end with fairly simliar words:\n",
    "```\n",
    "print(novel_list[5][\"text\"][-300:-1])\n",
    "\n",
    "acility:\n",
    "\n",
    "  http://www.gutenberg.org\n",
    "\n",
    "This Web site includes information about Project Gutenberg-tm,\n",
    "including how to make donations to the Project Gutenberg Literary\n",
    "Archive Foundation, how to help produce our new eBooks, and how to\n",
    "subscribe to our email newsletter to hear about new eBooks.\n",
    "```\n",
    "To get rid of these words, we can use *regular expressions* or *regexs*. A regular expression is a special kind of search term, that allows you to look for text in a very precise and flexible way. Execute the cell below to see how the regular expression `Kerryn.{0,10}Phelps` can be used to find all examples of the name `Kerryn Phelps` in a sentence, even if her middle name `Lyndel` is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The old sentence was: \"Kerryn Lyndel Phelps is the new member for Wentworth. Most people just call her Kerryn Phelps.\"\n",
      "And the new sentence is: \"Tony Abbott is the new member for Wentworth. Most people just call her Tony Abbott.\"\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "my_regex = re.compile('Kerryn.{0,10}Phelps') # first 'compile' your regular expression\n",
    "\n",
    "# Here is the sentence we would like to work with:\n",
    "sentence = \"Kerryn Lyndel Phelps is the new member for Wentworth. Most people just call her Kerryn Phelps.\"\n",
    "\n",
    "# Apply my_regex to our sentence, subbing all matches for the new phrase 'Tony Abbott'\n",
    "new_sentence = my_regex.sub('Tony Abbott', sentence)\n",
    "\n",
    "print(f'The old sentence was: \"{sentence}\"')\n",
    "print(f'And the new sentence is: \"{new_sentence}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Let's break down that regular expression:\n",
    "\n",
    "* `Kerryn`: this matches the exact phrase 'Kerryn', and is case-sensitive (it would not match 'kerryn')\n",
    "* `.`: the period is a special wildcard in a regex. It matches any character at all.\n",
    "* `{0,10}`: this says, 'look for 0-10 of the preceding character'. Since the preceding character in our regex was `.`, this means that the regex will match the word 'Kerryn', followed by 0-10 of any other character.\n",
    "* `Phelps`: this matches the exact phrase 'Phelps', and is case-sensitive.\n",
    "\n",
    "All together, the regular expression `Kerryn.{0,10}Phelps` looks for the exact words 'Kerryn' and 'Phelps' seperated by 0-10 of any other character.\n",
    "\n",
    "Now you are going to create two regular expressions, one that can strip away the boilerplate at the start of a Project Gutenberg ebook, and one that can strip away the boilerplate at the end.\n",
    "\n",
    "Every Project Gutenberg ebook begins with some metadata. When the metadata is over, a sentence of the following kind appears:\n",
    "```\n",
    "*** START OF THE PROJECT GUTENBERG EBOOK JANE EYRE ***\n",
    "```\n",
    "Every Project Gutenberg ebook ends with a licence allowing you to use it. You know when the book has ended and the licence has begun because of a sentence like this:\n",
    "```\n",
    "*** END OF THE PROJECT GUTENBERG EBOOK JANE EYRE ***\n",
    "```\n",
    "\n",
    "### Exercise 4.1: Clean away the metadata at the start of each file\n",
    "\n",
    "Use the cell below to create your first regex, which will strip away the metadata at the beginning. Some tools you can use:\n",
    "\n",
    "* `[A-Z]` will match any capital letter\n",
    "* `.` will match any character\n",
    "* `*` is a special character in a Python regex. To look for actual asterisks, you will need to type `\\*`.\n",
    "* `{m,n}` will match the preceding character m-n times. `{n}` will match it exactly n times. *Hint:* if you want to find three of the same character in a row, use `{3}`\n",
    "* `+` will match the preceding character 1 or more times.\n",
    "* `\\A` matches the start of a string (NB: in our dataset, the text of each novel is a single string)\n",
    "* You can match a space by simply typing a space: ` `.\n",
    "\n",
    "In the cell below, you can come up with each part of your regex seperately. I will deal with the `re.compile()` part for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My complete start_regex is: \\A.+\\*{3} {0,2}START OF.{0,100}\\*{3}.\n",
      "\n",
      "The metadata of erewhon.txt is:\n",
      "\n",
      " ﻿The Project Gutenberg eBook, Erewhon, by Samuel Butler\n",
      "\n",
      "\n",
      "This eBook is for the use of anyone anywhere at no cost and with\n",
      "almost no restrictions whatsoever.  You may copy it, give it away or\n",
      "re-use it under the terms of the Project Gutenberg License included\n",
      "with this eBook or online at www.gutenberg.net\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Title: Erewhon\n",
      "\n",
      "Author: Samuel Butler\n",
      "\n",
      "Release Date: March 20, 2005  [eBook #1906]\n",
      "\n",
      "Language: English\n",
      "\n",
      "Character set encoding: ISO-646-US (US-ASCII)\n",
      "\n",
      "\n",
      "***START OF THE PROJECT GUTENBERG EBOOK EREWHON***\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "start_1 = '\\A'   # Find the start of the string\n",
    "start_2 = '.+'   # Match one or more of any character\n",
    "start_3 = '\\*{3}'   # Match the exact phrase '***': i.e. match three asterisks (see hint above)\n",
    "start_4 = ' {0,2}'   # Match 0-2 spaces\n",
    "start_5 = 'START OF'   # Match the exact phrase 'START OF' NB: the licence at the end begins with 'START:'\n",
    "start_6 = '.{0,100}'   # Match 0-100 of any character \n",
    "start_7 = '\\*{3}'   # Match three asterisks again\n",
    "\n",
    "# END OF YOUR CODE\n",
    "\n",
    "start_regex = re.compile(start_1 + start_2 + start_3 + start_4 + start_5 + start_6 + start_7, flags = re.DOTALL)\n",
    "\n",
    "print(f'My complete start_regex is: {start_regex.pattern}.\\n')\n",
    "print(f'The metadata of {novel_list[0][\"title\"]} is:\\n\\n {start_regex.search(novel_list[0][\"text\"]).group(0)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Exercise 4.2: Clean away the licence at the end of each file\n",
    "\n",
    "Now you need to do the same for the licence at the end. This regex is much simpler. You simply need to find the phrase `*** END OF` and all the letters that come after it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My complete end_regex is: \\*{3} {0,2}END OF.+.\n",
      "\n",
      "The licence of corinne.txt is:\n",
      "\n",
      " *** END OF THIS PROJECT GUTENBERG EBOOK CORINNE; OR, ITALY***\n",
      "\n",
      "***** This file should be named 52077-0.txt or 52077-0.zip *****\n",
      "This and all associated files of various formats will be found in:\n",
      "        http://www.gutenberg.org/5/2/0/7/52077/\n",
      "\n",
      "Produc...\n",
      "\n",
      "\n",
      "...\n",
      "\n",
      "Most people start at our Web site which has the main PG search\n",
      "facility: www.gutenberg.org\n",
      "\n",
      "This Web site includes information about Project Gutenberg-tm,\n",
      "including how to make donations to the Project Gutenberg Literary\n",
      "Archive Foundation, how to help produce our new eBooks, and how to\n",
      "subscribe to our email newsletter to hear about new eBooks.\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "end_1 = '\\*{3}'    # Match three asterisks\n",
    "end_2 = ' {0,2}'    # Match 0-2 spaces\n",
    "end_3 = 'END OF'    # Match the exact phrase 'END OF'\n",
    "end_4 = '.+'    # Match one or more of any character\n",
    "\n",
    "# END OF YOUR CODE\n",
    "\n",
    "end_regex = re.compile(end_1 + end_2 + end_3 + end_4, flags = re.DOTALL)\n",
    "\n",
    "print(f'My complete end_regex is: {end_regex.pattern}.\\n')\n",
    "print(f'The licence of {novel_list[3][\"title\"]} is:\\n\\n {end_regex.search(novel_list[3][\"text\"]).group(0)[0:250]}...\\n\\n\\n...{end_regex.search(novel_list[3][\"text\"]).group(0)[-350:-1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now you have created two regexs that can find the extraneous text in each file, you can loop over your corpus and clean all your texts, using the same [re.sub()](https://docs.python.org/3.7/library/re.html#re.sub) method as we used above to sub Tony Abbott's name for Kerryn Phelps's. To delete any matches found by the regex, we can simply use `''` as the replacement.\n",
    "\n",
    "I have written the required code for you. Execute the cell below to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After applying our regexes, the beginning text of pride_and_prejudice.txt is:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Produced by Anonymous Volunteers\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PRIDE AND PREJUDICE\n",
      "\n",
      "By Jane Austen\n",
      "\n",
      "\n",
      "\n",
      "Chapter 1\n",
      "\n",
      "\n",
      "It is a truth universally acknowledged, that a single man in possession\n",
      "of a good fortune, must be in want of a wife.\n",
      "\n",
      "However little known the feelings or views of such a man may be on his\n",
      "first entering a...\n"
     ]
    }
   ],
   "source": [
    "for novel in novel_list:\n",
    "    text = novel['text'] # get the text of the novel\n",
    "    text = start_regex.sub('', text) # clean up the start\n",
    "    text = end_regex.sub('', text) # clean up the end\n",
    "    novel['text'] = text # replace the old text with the cleaned up text\n",
    "\n",
    "print(f'After applying our regexes, the beginning text of {novel_list[5][\"title\"]} is:\\n\\n{novel_list[5][\"text\"][0:300]}...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "There is still a bit more cleaning up we could do. There are lots of line breaks that make this text hard to read when printed to the screen, and there is still some extraneous metadata. But there is now far less messy text that might interfere with our analysis, and you now have the tools you need to work out how to clean your own texts in future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Section 5: Getting started with the NLTK\n",
    "\n",
    "The Python Natural Language Toolkit contains lots of nifty features. One that you will probably particuarly appreciate is its library of different tokenisers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1: ['just', '_tolerable_', '.”', '“', 'I', 'beg', 'you', 'would', 'not']\n",
      "Example 2: ['not', 'know', 'Jane', 's', 'disposition', 'as', 'you', 'do', 'But']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize, regexp_tokenize\n",
    "\n",
    "# Wordpunct_tokenize splits on puncutation\n",
    "example_1 = wordpunct_tokenize(novel_list[5]['text'])\n",
    "print(f'Example 1: {example_1[5835:5844]}')\n",
    "\n",
    "# Or you can define your own regex:\n",
    "example_2 = regexp_tokenize(novel_list[5]['text'], pattern = '\\w+')\n",
    "print(f'Example 2: {example_2[5835:5844]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Using NLTK's built in functions, it becomes very easy to tokenise your entire corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "for novel in novel_list:\n",
    "    novel['tokens'] = wordpunct_tokenize(novel['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Exercise 5.1: Find out how long each novel is\n",
    "\n",
    "Complete the for-loop in the cell below to calculate the word length (i.e. the number of tokens) of each novel in the corpus. Store the results in a `dict` of the following form:\n",
    "```\n",
    "results = {\n",
    "    'moby_dick.txt':200000,\n",
    "    'pride_and_prejudice.txt':100000,\n",
    "    etc.\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "results =      # Create the empty dict\n",
    "\n",
    "for novel in novel_list:\n",
    "    \n",
    "    # First retrieve the name of the novel from the dict\n",
    "    novel_title = \n",
    "    \n",
    "    # Then use the len() function on the novel's tokens to calculate the word count\n",
    "    novel_length = \n",
    "    \n",
    "    # Add it to the results dict (done for you):\n",
    "    results[novel_title] = novel_length\n",
    "\n",
    "# END OF YOUR CODE\n",
    "\n",
    "for key, value in results.items():\n",
    "    print(f'{key} is {value} words long.\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "To look at the word count of a particular novel, just type `results['title.txt']`.\n",
    "\n",
    "A very useful feature of the NLTK is the `Text()` object. If you convert your texts into a `Text()` object, it becomes very easy to do lots of different kinds of analysis on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from nltk.text import Text\n",
    "\n",
    "text_dict = {}\n",
    "\n",
    "for novel in novel_list:\n",
    "    title = novel['title']\n",
    "    nltk_text_object = Text(novel['tokens'])\n",
    "    text_dict[title] = nltk_text_object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "You can easily pull up all examples of a particular word from a text and see their context using the `.concordance()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 25 of 37 matches:\n",
      "there . If that ' s so , it ' s up a tree , straight . The ram - paddick ' s a\n",
      "of his wagon ; I hitched Bunyip to a tree , and mounted Fancy , and we cantere\n",
      "is pocket !\" \" O , go an ' bark up a tree , you mongrel !\" replied the war - m\n",
      " ' feller that watches from behine a tree -- keeps curs like Martin to do his \n",
      "nted . One of the fallers had left a tree nearly through when he went to dinne\n",
      "when he says ' mulga ', he means any tree except pine or currajong . Same ment\n",
      " day . Eucalypt , conifer , mimosa ; tree , shrub , heath , in endless diversi\n",
      "ears , I turned aside to inspect the tree . It was worth the trouble . The pin\n",
      "find a comprehensive allegory in the tree ; but I had scarcely turned away fro\n",
      "pure gladness of life ; endowed each tree with sympathy , respondent to her ow\n",
      "lean - spotted column of the leopard tree , creamy white on slate , from base \n",
      "had seen the man reclining under the tree ; and Rory nodded forgivingly when I\n",
      " little boomerangs over the same big tree , and we had been welted an equal nu\n",
      "sitting on a log , in the shade of a tree , on the north bank of the river , a\n",
      " The mustard seed has become a great tree , but the unclean fowls lodge in its\n",
      "e river , with the end fastened to a tree . When you haul the wire up out of t\n",
      "ou ' ll find the other end tied to a tree on this bank . Very complete rig . A\n",
      "ked my things in a convenient hollow tree , and started off down the river , f\n",
      "iced , not fifty yards away , a dead tree of twelve or fifteen tons displaceme\n",
      " forward end . In remarking that the tree was ong root , I merely mean to impl\n",
      "ht rather be viewed as a root with a tree attached than as a tree with a root \n",
      " root with a tree attached than as a tree with a root attached . This is the a\n",
      "he timber , I posted myself behind a tree , and waited as patiently as the mos\n",
      " in front of the buggy till a second tree offered its friendly cover . Jerry '\n",
      "as perfect silence for a minute . My tree was n ' t a large one , and the near\n"
     ]
    }
   ],
   "source": [
    "text_dict['such_is_life.txt'].concordance('tree')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "For a more subtle kind of analysis you can use the `common_contexts()` method. For this method, you must provide a `list` of two or more words, and find their common contexts in the text. An underscore represents any one of the words in the `list` you provide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jane_; jane_, jane_,\" ._came ._has ._mentioned ._. ._'\n"
     ]
    }
   ],
   "source": [
    "text_dict['jane_eyre.txt'].common_contexts(['Eyre','Rochester'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "*Note:* For the next cell, you must have `numpy` and `matplotlib` installed as instructed in the Getting Started.\n",
    "\n",
    "You can use the `.dispersion_plot` method to see how words are distributed through a text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "text_dict['corinne.txt'].dispersion_plot(['imagination','love','Italy','England'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Exercise 5.2: Play around\n",
    "\n",
    "You can find a full list of all the `Text()` methods in [the relevant section of the nltk website](http://www.nltk.org/api/nltk.html#nltk.text.Text). Have a play, see if you can find out anything interesting about your corpus of texts! The cell below is blank: do with it what you like..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (system-wide)",
   "language": "python",
   "metadata": {
    "cocalc": {
     "description": "Python 3 programming language",
     "priority": 100,
     "url": "https://www.python.org/"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}